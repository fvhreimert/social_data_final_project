# File: check_progress.py
import pandas as pd
import os

# --- Configuration (should match tmdb_data.py) ---
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, ".."))
OUTPUT_FILE_WITH_DATES = os.path.join(PROJECT_ROOT, "data", "movies_with_release_dates_status_filtered.parquet")
PROGRESS_FILE = os.path.join(PROJECT_ROOT, "data", "movies_processed_ids_status_filtered.txt")

# --- Load and Analyze ---
if not os.path.exists(OUTPUT_FILE_WITH_DATES):
    print(f"Output file {OUTPUT_FILE_WITH_DATES} not found. Cannot analyze.")
else:
    print(f"Loading data from: {OUTPUT_FILE_WITH_DATES}")
    try:
        df_results = pd.read_parquet(OUTPUT_FILE_WITH_DATES)
        print(f"Successfully loaded {len(df_results)} records.\n")

        if 'fetch_status' in df_results.columns:
            print("--- Fetch Status Summary ---")
            status_counts = df_results['fetch_status'].value_counts(dropna=False)
            print(status_counts.to_string()) # .to_string() ensures full output

            successful_count_status = status_counts.get("SUCCESS", 0)
            print(f"\nTotal with 'SUCCESS' status: {successful_count_status}")

            # Further check for actual dates among 'SUCCESS'
            if successful_count_status > 0:
                successful_mask = (df_results['fetch_status'] == "SUCCESS") & df_results['release_date_full'].notna()
                successful_with_date_df = df_results[successful_mask]
                num_successful_with_date = len(successful_with_date_df)
                print(f"Total 'SUCCESS' with a non-NA release_date_full: {num_successful_with_date}")

                if num_successful_with_date > 0:
                    # Check how many parse to valid datetimes
                    dates_converted = pd.to_datetime(successful_with_date_df['release_date_full'], errors='coerce')
                    num_valid_datetime_objects = dates_converted.notna().sum()
                    print(f"Total 'SUCCESS' that converted to valid datetime objects: {num_valid_datetime_objects}")
            print("--- End of Status Summary ---\n")
        else:
            print("Column 'fetch_status' not found in the results file.")

        # --- Progress File Info ---
        if os.path.exists(PROGRESS_FILE):
            with open(PROGRESS_FILE, 'r') as f:
                processed_ids_count = sum(1 for _ in f) # Efficient way to count lines
            print(f"Total IDs attempted (lines in progress file '{os.path.basename(PROGRESS_FILE)}'): {processed_ids_count}")
        else:
            print(f"Progress file '{PROGRESS_FILE}' not found.")

        # --- Optional: Estimate remaining (if you know total candidates) ---
        # This part requires knowing how many movies passed your initial content filters
        # You can get this number from the output of tmdb_data.py when it starts
        # (e.g., "Movies to consider for API processing after content filters: XXXXX")
        # Let's assume you note this number down, e.g., TOTAL_CANDIDATES_AFTER_FILTERS
        # Example: TOTAL_CANDIDATES_AFTER_FILTERS = 106016 (from your log)
        
        # This is an approximation, as "processed_ids_count" includes all attempts,
        # not just those that passed initial content filters if the progress file is from older runs
        # with different filters. A more accurate "remaining" would compare against the
        # 'title_ids_to_process' list generated by tmdb_data.py at the start of its run.

        # For a simpler view based on just total entries in main df:
        if 'fetch_status' in df_results.columns:
            processed_in_df = df_results['fetch_status'].notna().sum()
            total_in_df = len(df_results)
            remaining_to_attempt_in_df = total_in_df - processed_in_df
            print(f"\nTotal records in output Parquet: {total_in_df}")
            print(f"Records with a fetch_status (attempted and saved): {processed_in_df}")
            print(f"Records in Parquet without a fetch_status (potentially unattempted or from before status tracking): {remaining_to_attempt_in_df}")


    except Exception as e:
        print(f"Error loading or analyzing Parquet file: {e}")